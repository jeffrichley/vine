# ğŸ§ª {{ project_name }} Project Testing Rules

This document defines **precise, enforceable rules** for writing tests in {{ project_name }} projects.
It's designed to:

* Ensure consistent, maintainable, high-quality tests
* Balance fast execution with deep coverage
* Guide humans and AI contributors equally

---

## âœ… Core Principles

* Test **WHAT** the system does, not **HOW** it does it. Focus on public behavior, not internal implementation. Tests should fail when behavior is incorrect, not just when implementation changes.
* All production code must have tests. No exceptions.
* Tests must be readable, reliable, and relevant.
* All code should maintain **â‰¥90% line and branch coverage**, unless explicitly exempted.
* Tests are tiered by scope and purposeâ€”each type of test has a specific role in verifying the system.
* They must be placed in the correct subdirectory (e.g., unit/, integration/, e2e/) and tagged with the appropriate @pytest.mark.* marker to indicate their type and execution scope.
* This ensures test suites are fast, maintainable, and logically organized, and allows selective running in CI pipelines or during development.

### ğŸ” Example Directory + Marker Pairings

| Directory              | Purpose                            | Marker                 |
|------------------------|------------------------------------|------------------------|
| `tests/unit/`          | Fast, isolated function/class tests | `@pytest.mark.unit`    |
| `tests/integration/`   | Cross-module or DB/file system I/O  | `@pytest.mark.integration` |
| `tests/functional/`    | High-level, multi-layer behavior    | `@pytest.mark.functional`  |
| `tests/e2e/`           | End-to-end user flows or API         | `@pytest.mark.e2e`     |
| `tests/performance/`   | Load/latency testing                | `@pytest.mark.performance` |


---

## âœ… Test Structure Rules

### âœ… Function Naming

* Use: `test_<thing_under_test>_<expected_behavior>`
* Examples:

```python
def test_export_method_creates_output_file():
def test_register_raises_on_duplicate():
def test_yaml_loader_parses_valid_input():
```

### âœ… Docstrings

* Every test must have a 1-line docstring explaining **what is being tested**.

```python
def test_register_raises_on_duplicate():
    '''Test that register() raises ValueError if a name is already registered.'''
```

### âœ… Test Structure: Arrangeâ€“Actâ€“Assert

Tests must follow the Arrangeâ€“Actâ€“Assert structure:

```python
def test_register_raises_on_duplicate():
    '''Raises if duplicate name is registered.'''

    # Arrange
    registry = Registry()

    # Act
    with pytest.raises(ValueError):
        registry.register("foo", {})

    # Assert
    assert "foo" in registry.names
```

Use blank lines between phases. One clear action ("Act") per test. Never merge all three steps into one block.

---

## ğŸ”¬ What to Test

### âœ… Focus on:

* Public APIs
* Inputs â†’ Outputs
* Behavioral contracts (e.g., "should raise on duplicate")

### âŒ Avoid testing:

* Private methods (`_helper_func`) unless reused and complex
* Third-party library internals
* Logic already covered by other tests

---

## ğŸ§ª Assertion Rules

### âœ… Assert:

* Return values
* Exceptions (with `pytest.raises`)
* Side effects (state change, function calls)

### âŒ Anti-patterns:

* `assert True`
* Assertions without meaningful outcomes

### âŒ Avoid Fragile Static Assertions

```python
assert len(data) == 7  # Fragile
```

Use dynamic or relational assertions:

```python
assert len(data) > 0
assert "key" in data
```

---

## ğŸ§± Mocking Rules

### âœ… Mock external systems:

* Filesystem (`os`, `pathlib`, etc.)
* Time (`datetime`, `sleep`)
* Networking (`httpx`, `requests`)
* Databases and ORMs
* Cloud SDKs

### âœ… How to mock:

* Use `@patch` decorators for readability
* Prefer `MagicMock()` with `spec=...` for object mocks
* Assert behavior (not just call existence)

### âŒ Never mock:

* The method you're testing
* Internal functions without a compelling reason

### âœ… Do mock your code ONLY IF you justify it

> If using Cursor/AI, request permission before mocking internals:

```text
INTERNAL MOCK REQUEST:
I want to mock `TimelineBuilder._build_layers()` because it's slow and already tested elsewhere.
```

Use interface segregation or fakes before mocking internal flow.

---

## ğŸ§ª Pytest Markers

Use markers for runtime scoping and CI targeting:

```python
@pytest.mark.unit
def test_basic_math():
    ...
```

Define all markers in `pytest.ini` or `pyproject.toml` to avoid warnings.

---

## ğŸ“Š Coverage Expectations

* Maintain **â‰¥90%** coverage (lines + branches)
* Use `pytest-cov` and enforce `fail_under = 90`
* Allow `# pragma: no cover` only when:
  - Platform-specific
  - Logically unreachable branches

Security- or mission-critical logic (e.g., reward functions, reducers): **target 100%**

---

## ğŸ§¬ Mutation Testing (Advanced)

Use `mutmut` or `cosmic-ray` to verify test strength.

Required for:
- Reward functions
- Decision graphs
- Flow control systems

---

## ğŸ“ Naming Conventions

* Files: `test_<module>.py`
* Classes: `Test<ClassName>`
* Methods: `test_<expected_behavior>_when_<context>()`

```python
def test_returns_403_when_user_is_unauthenticated():
```

---

## ğŸ” Fixtures & Setup

* Shared: `conftest.py`
* Use factory libraries (e.g., `FactoryBoy`, `pydantic-factories`)
* Use `scope="session"` for containers or expensive setup
* Avoid `autouse=True` unless necessary

---

## ğŸ§¹ Warning Suppression

Scoped only:

```python
warnings.filterwarnings("ignore", category=UserWarning, module="pytest.*")
```

Avoid global filters like `ignore::DeprecationWarning`

---

## ğŸ”¬ Best Practices

### âœ… Test happy and sad paths:

```python
assert builder.export("good.json").success

with pytest.raises(FileNotFoundError):
    builder.export("missing.png")
```

### âœ… Keep runtime <200ms

### âœ… Use helper fixtures for setup

---

## ğŸ“ Directory Structure

| Directory            | Purpose                            |
|----------------------|------------------------------------|
| `tests/unit/`        | Fast, logic-only tests             |
| `tests/integration/` | Module-to-module testing           |
| `tests/functional/`  | System behavior with mocks         |
| `tests/e2e/`         | Full-path tests from entry to output |
| `tests/performance/` | Runtime, throughput, memory        |

---

## ğŸš« Anti-patterns

| Bad Practice                  | Instead...                          |
|------------------------------|-------------------------------------|
| `assert True`                | Write meaningful assertions         |
| Over-patching                | Use `@patch` only where needed      |
| Mocking internal logic       | Inject or test real implementations |
| Testing mocks instead of logic | Assert true behavior and outputs  |
| Test state dependent on config | Inject config with fixtures       |

---

## ğŸ”§ CI Strategy

Split jobs by marker:

```yaml
- name: Unit Tests
  run: pytest tests/unit -m unit

- name: Integration Tests
  run: pytest tests/integration -m integration

- name: End-to-End Tests (nightly)
  run: pytest tests/e2e -m e2e
```

Use `@pytest.mark.slow` for >5s and run them selectively.

---

## âœ… Summary

Tests must be:

- **Isolated** â€“ no hidden state or side effects
- **Fast** â€“ sub-second preferred
- **Focused** â€“ each test proves one thing
- **Descriptive** â€“ names and docstrings must explain intent
- **Strategic** â€“ only mock external boundaries 